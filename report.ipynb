{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 3 Mobile Robotics Report\n",
    "\n",
    "**Group Members**: Luca Engel, Marc Nassif,  Giuseppe De Carlo,  Giada Ehrlich\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup](#2-setup)\n",
    "3. [Implementation](#2-implementation)\n",
    "      - 3.1 [Vision](#3-global-navigation)\n",
    "         -  [Marker Detection](#31-marker-detection)\n",
    "         -  [Grayscale Conversion and Thresholding](#32-thresholding)\n",
    "         - [2D Grid Representation](#31-2D-representation)\n",
    "         - [Dynamic Updates](#31-dynamic-updates)\n",
    "      - 3.2 [Global Navigation](#32-vision)\n",
    "         -  [Dijkstra Algorithm](#41-Djkstra-algorithm)\n",
    "         -  [Path Discretization](#42-path-discretization)\n",
    "         -  [Integration with Vision System](#42-integration-vision)\n",
    "      - 3.3 [Motion Control](#5-motion-control)\n",
    "         -  [Implementation Details](#51-implementation-details)\n",
    "         -  [Functions](#52-functions)\n",
    "      - 3.4 [Local Navigation](#6-local-navigation)\n",
    "         -  [Implementation Details](#61-implementation-details)\n",
    "         -  [Functions](#62-functions)\n",
    "      - 3.5 [Kalman Filter](#7-kalman-filter)\n",
    "         - [Wheel Speed Measurements](#71-wheel-speed-measurements)\n",
    "         - [Prediction and Update Step](#72-prediction-update-step)\n",
    "         - [Handling Orientation Jumps](#72-handling-orientation-jumps)\n",
    "         \n",
    "4. [Results](#4-results)\n",
    "   - 4.1 [Summary](#41-summary)\n",
    "   - 4.2 [Videos](#42-video)\n",
    "   \n",
    "5. [References](#4-references)\n",
    " \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mobile robotics, fusing computer vision and sensor-based feedback is essential to establish reliable global navigation algorithms, motion control strategies, and sensor-based local obstacle avoidance to enable robots to intelligently and reliably navigate through dynamic environments. This report describes our effort to develop a solution that enables the Thymio robot to traverse a terrain containing obstacles and to reach a goal.\n",
    "\n",
    "Our project integrates multiple technologies to improve the navigation precision. With the help of a video feed provided by a camera, computer vision is used to detect the map, the obstacles in the map, the Thymio robot, and the goal. Based on this information, a 2d grid of cells is constructed. The global navigation aspect employs the Dijkstra algorithm to calculate the shortest path from the Thymio to the goal, accounting for dynamically changing events, such as kidnapping where the Thymio or the goal are displaced. The motion control mechanism guided the Thymio along the calculated path which is discretized into intermediate points, facilitating a smooth and precise traversal. For local obstacle avoidance, the Thymio relies on its proximity sensors, allowing it to detect and navigate around obstacles not identified by the computer vision part. This ensures robust navigation, even when visual data is limited. The implementation of the Kalman filter for position and orientation estimation plays a pivotal role in this project. The filter combines information from the computer vision part and the wheel speeds of the Thymio to improve robustness of the navigation. The filter is even capable of functioning when the camera feed is obscured for a while, showing the robustness of our system.\n",
    "\n",
    "This report details the design, implementation, and performance of our solution and shows how the combination of sensor data can drastically improve the capabilities and precision in robotic navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries required: \n",
    "- NumPy\n",
    "- tmdclient\n",
    "- OpenCV\n",
    "- FilterPy\n",
    "\n",
    "### Method overview: \n",
    "\n",
    "Thymio initially reads a global map from processed images acquired through the webcam. It subsequently extracts a path through global navigation and adheres to this route unless faced with kidnapping or local navigation triggers.\n",
    "\n",
    "The provided schema illustrates the code structure implemented for this purpose. In cases where the camera becomes obscured, the robot continues to follow the pre-defined global path planning until the camera sensing functionality is restored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./report_images/implementation_structure.png\" alt=\"Image Alt Text\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "*implementation strucure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Environment\n",
    "\n",
    "\n",
    "Webcam  : AUKEY 1080P\n",
    "\n",
    "Map :\n",
    "- White background \n",
    "- Global obstacles: black \n",
    "- Local obstacles: small 3-dimensional objects\n",
    "- Markers: ArUco Markers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./report_images/set_image.png\" alt=\"Image Alt Text\" width=\"300\" height=\"400\"/>\n",
    "\n",
    "*Set Up*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vision system forms the baseline of the Thymio's understanding of the environment. Using the OpenCV library, the map is created in multiple steps, each of which extracting information from the camera feed, allowing our program to identify markers, determine their orientation, and recognize obstacles. In the following, our procedure will be shown with the example of the following image feed:\n",
    "\n",
    "![Simple Camera Feed](./report_images/image_processing_images/simple_camera_feed.png)\n",
    "*Unprocessed Camera Feed*\n",
    "\n",
    "\n",
    "##### Marker Detection\n",
    "We used OpenCV's ArUco marker detection algorithm to find the key elements and their orientation within the terrain, specifically, the Thymio robot, the goal, and the corners of the map. Based on this information, a transformation matrix is applied to the image to place the map corners into the corners of the image so that the map fills the entire video feed.\n",
    "\n",
    "![Processed Image with Map Filling the Entire Screen](./report_images/image_processing_images/normal_feed_img.png)\n",
    "*Processed Feed with Map Filling the Screen*\n",
    "\n",
    "##### Grayscale Conversion and Thresholding\n",
    "To simplify the object detection, we converted the camera feed to grayscale. As the objects are represented as black shaped, applying thresholding to the image allowed for easy extraction of their location in the environment.\n",
    "\n",
    "![Binary Image](./report_images/image_processing_images/binary_img.png)\n",
    "*Binary Image*\n",
    "\n",
    "The resulting image still contains some noise due to the black thymio sensors being detected as obstacles. This is resolved in the next step. \n",
    "\n",
    "##### 2D Grid Representation\n",
    "Based on the information obtained in the previous steps, a 2D grid of cells is created. Each cell stores information about its status, indicating whether it is free, occupied by the Thymio (green), the goal (blue), or an obstacle. \n",
    "\n",
    "After conversion, the 2D grid looks as follows:\n",
    "\n",
    "![Grid Map with obstacles](./report_images/image_processing_images/grid_map_with_islands_not_increased_object_size.png)\n",
    "*Grid Map with Obstacles*\n",
    "\n",
    "Here, still, the map contains the noise from the black sensors of the Thymio and the ArUco markers of which some are not entirely erased from the map. This is solved by removing all obstacles that are detected to be small islands which results in the following:\n",
    "![Grid Map with Islands Removed](./report_images/image_processing_images/grid_map_islands_removed_not_increased.png)\n",
    "*Grid Map with Islands Removed*\n",
    "\n",
    "\n",
    "Lastly, to account for the width of the Thymio, obstacles are artificially enlarged to ensure that the Thymio cannot drive too close to the obstacles. The resulting map looks as follows:\n",
    "\n",
    "![Grid Map with Object Size Increased](./report_images/image_processing_images/grid_map_islands_removed_objects_increased.png)\n",
    "*Grid Map with Object Size Increased*\n",
    "\n",
    "\n",
    "\n",
    "##### Dynamic Updates\n",
    "The grid is continuously updated with the positions of the Thymio and the goal within the map. These updates ensure that the Thymio can make informed decisions and enable recomputing the path after kidnapping events or motion control adjustments.\n",
    "\n",
    "This part lays the groundwork for successful navigation of the Thymio robot in the terrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Global Navigation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our global navigation system guides the Thymio robot throught the map by calculating the shortest path from its start location to the goal. This path is found with the Dijkstra algorithm. Similar to the vision, the navigation is also constructed in a multi-step approach.\n",
    "\n",
    "##### Dijkstra Algorithm\n",
    "The shortest path is calculated using the Dijkstra algorithm. The resulting path ensures that the Thymio can navigate through the environment while avoiding obstacles and ensures that it efficiently reaches the goal.\n",
    "\n",
    "##### Path Discretization\n",
    "To have the Thymio drive more smoothly, the computed path is discretized into a series of fewer cells. Each cell represents a direction change in the path. The Thymio receives these cells as intermediate goals allowing a more natural movement along the path and decreasing the number of sudden direction changes. The resulting path is marked red in the map and the direction changes purple. With this addition, the grid image looks as follows:\n",
    "\n",
    "![Image Alt Text](./report_images/image_processing_images/grid_map_with_path.png)\n",
    "\n",
    "*Grid Map Marked with Path and Direction Changes* \n",
    "\n",
    "##### Integration with Vision System\n",
    "Based on the information received from the vision system, the path is calculated and obstacles can be avoided. Additionally, kidnapping of the thymio and the goal can be handled. Whenever such a kidnapping occurs, the path is recomputed so that the goal can be reached.\n",
    "\n",
    "\n",
    "The combination of the Dijkstra algorithm, path discretization, and intermediate goal navigation, ensures a smooth and fast traversal of the map, even accounting for sudden events such as kidnapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Motion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "The motion control is designed to control the speed and the direction of the Thymio. It was defined as a class and performs the basic movements of the robot. The class is used during the all process, except when the Thymio needs to avoid an obstacle and it's the local navigation that is active.\n",
     "\n",
     "The motion control handles 3 states of movement of the robot :\n",
     "1. STOP: Stop the robot to move during the process or when it reached the goal.\n",
     "2. ROTATE: Before moving to the next point, the Thymio will align is direction on it. When it's done the robot will pass in MOVE mode.\n",
     "3. MOVE: When the Thymio has the confirmation that it's align, it will start to move to reach the next point. This process is controlled by a PI controller to adjust the trajectory of the Thymio.\n",
     "\n",
     "Now we will describe the main movement of our robot, the ROTATE and the MOVE and how tey work.\n",
     "\n",
     "#### ROTATE\n",
     "![Image Alt Text](./report_images/BOMR_Rotation.PNG)\n",
     "\n",
     "*ROTATE mode illustration*\n",
     "As we observe with the picture, the ROTATE mode will calculate the error between the desired angle and the Thymio angle. When it's done, the Thymio will find the sorthest path to use to be aligned with the desired angle and will rotate on itself to start the correction. When the Thymio has reached the correct angle, the camera will inform it and will send the command MOVE to start the movement to the next point.\n"
     "![Image Alt Text](./report_images/BOMR_PI.PNG)\n",
     "\n",
     "*MOVE mode illustration*\n",
     "After that the Thymio is aligned with the desired angle, the PI controller will handle the movement into the next point. During it, the regulation will work to keep the direction of the Thymio in the threshold angle. When the Thymio is in the threshold zone, *the green cone*, the same speed is implemented in both wheels. When the thymio is out of the cone, the PI controller will correct the direction by increasing the speed of one wheel and decreasing the speed of the other one. The regulation work until the error is inside treshold zone to avoid oscillation on the desired angle and constatly using the controller.\n",
     "Like explained before, the PI controller is used during all the movement of the robot, except when it enters in obstacle avoidance who is handeld by the *Local Navigation*. The tuning of our regulator, *Kp* and *Ki*, has been done in experiment way. We observed how the robot was correcting its error with little gains and we increase our gains to have a smooth and dynmaic correction. We finally got good result with a *Kp = 3* and a *Ki = 0.2*.\n",
     "We implemented a parameter *sum_error_max* to avoid drift of our wheeels during the movement. The max value was fined by an experiment way, like the regulator parameters.\n",
     "#### Program of the movement\n",
     " Finally, you can observe, with the next schematic, how the motion control of our Thymio is configure to succeed its mission.\n",
     "![Image Alt Text](./report_images/BOMR_MotionStep.PNG)\n",
     "\n",
     "*Schematic of the movement*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Navigation depends on the status of the proximity sensors. Since, in our project, the robot does not move backwards, the sensors at the back of the Thymio are ignored. An instance of the `LocalNavigation` class tracks the variables relevant to local navigation and is instantiated at program startup. On every iteration of the primary while True loop, the proximity sensors are polled, and their values are sent to the `judge_severity()` method, which assigns a danger level to each sensor which increases when obstacles are nearer. The maximum danger level of the sensors is assigned to the local navigation's `danger_state`. The DangerState enum contains the thresholds, found by experimentation. When the Thymio is in an unsafe state, Global Navigation is ignored, and a status variable (`local_nav.state`) is modified to track in which phase of local navigation the Thymio is.\n",
    "\n",
    "Local Navigation is split in two parts:\n",
    "- Rotation: The Thymio determines a direction to turn, based on the path's next turn direction, in order to go on the outside of the turn. Using Dijkstra navigation means the path typically sticks to global obstacles, therefore, going on the outside works well to avoid these obstacles. Once a direction is chosen, the Thymio rotates in place until it detects no more obstacles around it. Once it is clear, it still rotates for a couple iterations* to add some slack and avoid turning too close to the obstacle and grazing it.\n",
    "\n",
    "- Circling: The Thymio's orientation being now fixed, we can start circling the obstacle, by running the motors forward with a slight offset between them, making a wide circle around the obstacle. If the Thymio sensors detect an obstacle again, we go back to Turning, but using the same direction decided in the first place: this means that long obstacles can be avoided by the Thymio drawing circles along its edge. Circling ends once the Thymio detects that it is back on the Global Navigation path, which is bound to happen, as pathfinding won't create looping paths. Once we are back on path, the thymio recomputes which checkpoints (turning points) it still needs to, chooses the next one and yields back to Global Navigation.\n",
    "\n",
    "It therefore acts as a simple Finite State Machine, illustrated in the image below:\n",
    "\n",
    "![Image Alt Text](./report_images/image_processing_images/local_navigation_fsm.png)\n",
    "\n",
    "\n",
    "*Local Navigation Finite State Machine illustration*\n",
    "\n",
    "The START State allows Global Navigation to steer the Thymio freely, so long as no obstacle is detected by the sensors, which will drive a state change.\n",
    "\n",
    "If the Thymio's marker is obscured during Local Navigation, it will still move using the Kalman Filter to estimate position. Once sight is restored, if the Thymio's position has changed too much, the Thymio will recompute the optimal path and use that to move. This is necessary, as it allows us to kidnap the Thymio during Local Navigation, and have it handle well. This situation also happens in the case of tall obstacles, and typically the Thymio's new path will not cross the obstacle, but even if it does, circling will work well as it will not go back behind the obstacle.\n",
    "\n",
    "*The counters on the Thymio's rotation are staggered by Global Navigation's `wait_for_variables`. While this is not exact, we do not need an exact delay as we are not aiming for perfect navigation, we just need a little bit of slack in this application!\n",
    "\n",
    "The local navigation path is illustrated below:\n",
    "\n",
    "![Image Alt Text](./report_images/image_processing_images/grid_map_with_local_avoidance.png)\n",
    "\n",
    "*Grid Map Marked with Local Avoidance Path*\n",
    "\n",
    "The Thymio detects the object (here in blue) and deviates from the Global Path (in red), choosing to go on the *outside* of the path's turn, thus drawing a circle, here in orange.\n",
    "\n",
    "Please note however that this image is never generated, as the camera and the computer are never aware of the local obstacle, this is only an illustration of a path possibly taken during local navigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Kalman filter implementation serves a critical role in the position and orientation estimation of our Thymio. It uses the KalmanFilter library of pythons filterpy package and combines the wheel motor speed of the Thymio with the information provided by the computer vision part to increase accuracy.\n",
    "\n",
    "##### Wheel Speed Measurements\n",
    "The wheel speed measurements are used for both estimating the position and orientation of the Thymio. To predict the new orientation, the angular velocity is calculated based on the difference of wheel speeds and a baseline minimum period for a 360° turn of the Thymio. Additionally, the wheel speeds are used to estimate the new position of the Thymio. \n",
    "\n",
    "##### Camera input\n",
    "As the camera input already is provided in grid coordinates by the computer vision, there is no additional processing needed for it.\n",
    "\n",
    "##### Prediction and Update Step\n",
    "We use the wheel speed measurements to perform the prediction step of the Kalman filter and use the computer vision information in the update method. With the combination of these two informations, the Kalman filter provides an accurate estimation of the position and orientation of the Thymio robot.\n",
    "\n",
    " ##### Handling Orientation Jumps\n",
    "As the provided orientation input from our computer vision module is a float between 0 and 2π, depending on the Thymio's orientation, there may be jumps in the estimated orientation from 0 to 2π and vice versa. This leads to large jumps in the overall estimation of the orientation. To combat that, we do not limit the predicted orientation to [0, 2π] but let it be any number and scale using the mod operation only when the navigation part accesses it. This dramatically improved the orientation estimation of our filter.\n",
    "\n",
    "##### Adaptability to Vision Feed Cut-Off\n",
    "Whenever the camera feed is unavailable, the Kalman filter continues to estimate the postition and orientation based solely on the predictions derived from the wheel speed measurements. This allows the Thymio to maintain knowledge of its position and orientation even when external measurements are unavailable\n",
    "\n",
    "The Kalman filter's ability to combine information from wheel speed measurements and vision data drastically increases the robustness of our navigation and even allows for accurate driving when no computer vision data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Details in the choice of matrices \n",
    "the matrices for the kalman filter are the following:\n",
    "- F = state transition matrix\n",
    "- H = the measurement function  \n",
    "- R =  the measurement noise covariance\n",
    "- Q = process noise covariance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "& F=\\left[\\begin{array}{llll}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right] \\quad R=\\left[\\begin{array}{llll}\n",
    "\\sigma_x^2 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_y^2 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_\\theta^2 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right] \\\\\n",
    "& H=\\left[\\begin{array}{llll}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right] \\quad Q=\\left[\\begin{array}{llll}\n",
    "\\sigma_x^2 & 0 & 0 & 0 \\\\\n",
    "0 & \\sigma_y^2 & 0 & 0 \\\\\n",
    "0 & 0 & \\sigma_\\theta^2 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The variances were determined through experimental considerations. Regarding the process noise covariance, we assigned a variance of 0.001 to both the x and y directions, reflecting the comparable uncertainty associated with forward movement in these dimensions. In contrast, the rotation variance was set to 1, acknowledging a higher level of uncertainty when determining orientation during movement.\n",
    "\n",
    "\n",
    "As for the measurement noise covariance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image processing and vision components proved to be highly robust, consistently delivering accurate results in detecting obstacles, the Thymio robot, and the goal. The implementation of live camera feed utilizing ArUco markers was particularly precise, even enabling successful recovery from simulated kidnapping scenarios.\n",
    "\n",
    "Concerning navigation, the path derived through the Dijkstra algorithm demonstrated sufficient optimization, allowing the robot to traverse from the starting point to the goal position efficiently. The motion control mechanisms effectively ensured that the Thymio adhered to the designated path.\n",
    "\n",
    "\n",
    "(The biggest problem that we encountered was ..\n",
    "\n",
    "\n",
    "(In terms of filtering, we were able to obscure the camera and still follow the path with good precision, so the implementation was successfull. )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T18:48:25.726354200Z",
     "start_time": "2023-12-03T18:47:52.968791200Z"
    }
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\EPFL\\Mobile Robotics\\Projet\\MobRob_Project-1\\global_nav\\GlobalNavigation.py:423\u001b[0m\n\u001b[0;32m    419\u001b[0m         aw(node\u001b[39m.\u001b[39mset_variables(motion_control\u001b[39m.\u001b[39mmotors(left_speed, right_speed)))\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     client \u001b[39m=\u001b[39m ClientAsync()\n\u001b[0;32m    424\u001b[0m     node \u001b[39m=\u001b[39m aw(client\u001b[39m.\u001b[39mwait_for_node())\n\u001b[0;32m    426\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PST\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tdmclient\\clientasync.py:42\u001b[0m, in \u001b[0;36mClientAsync.__init__\u001b[1;34m(self, node_class, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, node_class\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39msuper\u001b[39;49m(ClientAsync, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_class \u001b[39m=\u001b[39m node_class \u001b[39mor\u001b[39;00m tdmclient\u001b[39m.\u001b[39mClientAsyncCacheNode\n",
      "File \u001b[1;32mc:\\Users\\PST\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tdmclient\\client.py:98\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, tdm_transport, password, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_transport \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTDM \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_port\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_handshake(password)\n",
      "File \u001b[1;32mc:\\Users\\PST\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tdmclient\\client.py:112\u001b[0m, in \u001b[0;36mClient.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnectionWS(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_addr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm_ws_port)\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtdm \u001b[39m=\u001b[39m TDMConnection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_addr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtdm_port)\n",
      "File \u001b[1;32mc:\\Users\\PST\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tdmclient\\tcp.py:104\u001b[0m, in \u001b[0;36mTDMConnection.__init__\u001b[1;34m(self, host, port, debug)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39msendall(b)\n\u001b[1;32m--> 104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mio \u001b[39m=\u001b[39m TCPClientIO(host, port)\n\u001b[0;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m debug\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PST\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tdmclient\\tcp.py:96\u001b[0m, in \u001b[0;36mTDMConnection.__init__.<locals>.TCPClientIO.__init__\u001b[1;34m(self, host, port)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, host, port):\n\u001b[0;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39msocket(socket\u001b[39m.\u001b[39mAF_INET, socket\u001b[39m.\u001b[39mSOCK_STREAM)\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((host, port))\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "%run ./global_nav/GlobalNavigation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources \n",
    "\n",
    "https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
